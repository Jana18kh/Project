{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "trusted": true
      },
      "source": [
        "## The goal of collecting this dataset:\n",
        "The goal of collecting the Student Stress Factors dataset is to conduct a comprehensive analysis of the factors contributing to student stress, with a focus on classifying students into different stress levels and clustering them based on common stress-related characteristics. By examining variables such as academic workload, personal life, social pressures, and mental health, this dataset aims to identify patterns and relationships that can classify students’ stress levels. Additionally, clustering techniques will be used to group students with similar stress profiles, which can provide insights for developing targeted strategies to reduce stress, improve well-being, and enhance academic performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The source of the dataset:\n",
        "https://www.kaggle.com/datasets/rxnach/student-stress-factors-a-comprehensive-analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('Dataset/StressLevelDataset(in).csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## General information about the dataset:\n",
        "Number of attributes: 21\n",
        "Number of objects: 1100\n",
        "Attribute types: All columns are integer types (int64)\n",
        "## General information about the dataset:\n",
        "Number of attributes: 21\n",
        "\n",
        "Number of objects: 1100\n",
        "\n",
        "Attribute types: All columns are integer types (int64)\n",
        "\n",
        "Class label: stress_level"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('Dataset/StressLevelDataset(in).csv')\n",
        "num_objects = len(df)\n",
        "attributes_info = pd.DataFrame({\n",
        "    'Attribute Name': df.columns,\n",
        "    'Data Type': df.dtypes.values\n",
        "})\n",
        "print(\"Number of attributes:\" ,len(df.columns))\n",
        "print()\n",
        "print(\"Attributes and their types:\")\n",
        "print(attributes_info)\n",
        "print()\n",
        "print(\"Number of objects: \",num_objects)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Check the Current Distribution of the Class Label:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check the current distribution of the class label\n",
        "\n",
        "# Use value_counts to get the count of each unique value in the 'stress_level' column\n",
        "# Set normalize=True to get the relative frequencies as percentages, multiplied by 100\n",
        "class_distribution = df['stress_level'].value_counts(normalize=True) * 100\n",
        "print(\"Class label distribution in the full dataset:\") # Print a message to describe the output\n",
        "print(class_distribution) # Display the distribution of the class labels in percentages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Graphs:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Bar char (Stress Level):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Display the class distribution (counts of each class label)\n",
        "print(class_distribution)\n",
        "plt.figure(figsize=(6, 4)) # Set up the figure size for the plot\n",
        "sns.countplot(x='stress_level', data=df, color='lightblue') # Use Seaborn's countplot to plot the distribution of the 'stress_level' column\n",
        "plt.title('Stress Level distribution') # Set the title of the plot\n",
        "plt.show() # Display the plot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This bar chart indicates that the dataset has an equal distribution of data across all stress levels, where 0 represents low stress, 1 stands for medium stress, and 2 indicates high stress. This balance is crucial for ensuring the integrity of analysis and predictions, as it prevents class imbalance that can distort results or cause predictive models to become biased. In an imbalanced dataset, models may disproportionately favor the majority class (e.g., \"low stress\"), leading to inaccurate and unfair predictions for underrepresented categories (e.g., \"high stress\"). The balanced representation in this dataset ensures more reliable and fair predictions, allowing for better understanding and intervention across all stress levels, thereby promoting effective strategies for managing student stress."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Pie chart (Social Support):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "data2 = df['social_support'].value_counts(normalize=True) * 100 # Calculate the percentage distribution of the 'social_support' column\n",
        "custom_colors = ['#ff9999', '#66b3ff', '#99ff99', '#ffcc99'] \n",
        "data2.plot.pie(autopct='%1.1f%%', figsize=(6, 6), startangle=90, colors=custom_colors) # Plot the percentage distribution as a pie chart\n",
        "plt.title('Percentage Distribution of Social Support') # Set the title of the pie chart\n",
        "plt.ylabel('') # Remove the y-axis label for a cleaner pie chart presentation\n",
        "plt.show() # Display the pie chart"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The pie chart reveals that the majority of students (over 70%) experience either low or high levels of social support, with the largest percentage (41.6%) feeling well-supported. This suggests that many students have access to strong social networks, which can play a critical role in their academic success and mental health. However, around 8% of students report having no support, which is concerning. A lack of social support can significantly impact a student’s ability to manage stress, maintain motivation, and succeed academically. Addressing this issue through targeted interventions, such as peer mentoring, counseling services, or group activities, could help those with little to no support build stronger connections and improve their overall well-being. Understanding these different levels of social support is key to developing strategies that support students’ academic performance and mental health."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Box plot (Bullying):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a box plot for the 'bullying' column in the dataset\n",
        "sns.boxplot(data=df['bullying'], color='lightblue')\n",
        "plt.title('Bullying Distribution') # Set the title for the plot\n",
        "plt.show() # Display the plot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The box plot shows the distribution of the \"bullying\" variable, with the interquartile range (IQR) captured by the box, and the 25th and 75th percentiles at the edges. The median bullying level is around 3, as indicated by the line inside the box. The whiskers extend to the minimum and maximum values, showing that most data points lie between 1 and 5, with no significant outliers. This distribution suggests that bullying incidents are relatively common in the population, with most students experiencing moderate levels of bullying. The lack of outliers indicates that extreme cases are rare, and interventions may need to focus on addressing the more typical experiences of bullying rather than isolated severe incidents. Understanding this spread is crucial for tailoring prevention and support programs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Histogram (Stress level, Academic performance)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6)) # Set the figure size for the histogram\n",
        "plt.hist(df['stress_level'], \n",
        "         bins=15,                 # Number of bins for the histogram\n",
        "         color='coral',          # Color for the Stress Level histogram\n",
        "         edgecolor='black',      # Color of the bin edges\n",
        "         alpha=0.6,              # Transparency level of the histogram\n",
        "         label='Stress Level',    # Label for the legend\n",
        "         histtype='stepfilled',   # Style of the histogram\n",
        "         linewidth=1.5)          # Width of the bin edges\n",
        "\n",
        "plt.hist(df['academic_performance'], \n",
        "         bins=15,                 # Number of bins for the histogram\n",
        "         color='darkblue',       # Color for the Academic Performance histogram\n",
        "         edgecolor='white',      # Color of the bin edges\n",
        "         alpha=0.6,              # Transparency level of the histogram\n",
        "         label='Academic Performance', # Label for the legend\n",
        "         histtype='stepfilled',   # Style of the histogram\n",
        "         linewidth=1.5)          # Width of the bin edges\n",
        "\n",
        "plt.title('Comparison of Stress Level and Academic Performance Distribution', fontsize=14, fontweight='bold')  # Title of the plot\n",
        "plt.xlabel('Value', fontsize=12)        # X-axis label\n",
        "plt.ylabel('Frequency', fontsize=12)    # Y-axis label\n",
        "plt.grid(True, linestyle='--', alpha=0.7)  # Style of the gridlines\n",
        "plt.legend(frameon=True, fancybox=True, shadow=True, loc='upper right', fontsize=11)  # Legend properties\n",
        "\n",
        "# Display the histogram\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Based on the histogram illustrating the relationship between stress level and academic performance, it is clear that students with higher academic achievement tend to exhibit elevated stress levels. This trend indicates that as students strive for better grades and higher academic standing, they often encounter increased pressure and demands associated with their studies. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Scater plot(Anxiety Level, Self esteem)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))  # Set the figure size for the scatter plot\n",
        "\n",
        "# Normalize the anxiety level for color mapping\n",
        "norm = plt.Normalize(df['anxiety_level'].min(), df['anxiety_level'].max())\n",
        "\n",
        "# Create a scatter plot with colors based on anxiety levels\n",
        "scatter = plt.scatter(df['anxiety_level'], df['self_esteem'], \n",
        "                      c=df['anxiety_level'],  # Color based on anxiety levels\n",
        "                      cmap='viridis',         # Colormap to use\n",
        "                      norm=norm,              # Normalize values for colormap\n",
        "                      alpha=0.7,             # Set transparency of points\n",
        "                      edgecolor='black')      # Outline color for points\n",
        "\n",
        "plt.title('Anxiety Level vs. Self esteem')  # Set the title for the scatter plot to describe the data being represented\n",
        "plt.xlabel('Anxiety Level')  # Label the x-axis to indicate that it represents anxiety levels\n",
        "plt.ylabel('Self esteem')  # Label the y-axis to indicate that it represents depression levels\n",
        "cbar = plt.colorbar(scatter) # Add a colorbar to indicate the mapping of colors to anxiety levels\n",
        "cbar.set_label('Anxiety Level')  # Label for the color bar\n",
        "\n",
        "plt.show() \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This plot illustrates a negative relationship between anxiety level and self-esteem, suggesting that as anxiety levels increase, self-esteem tends to decrease. This finding aligns with expectations in psychological studies, where heightened anxiety is often associated with lower self-worth and confidence. The observed pattern indicates a potential link between these variables in your data, highlighting the importance of addressing anxiety to promote healthier self-esteem levels. Further analysis on the correlation between anxiety and self-esteem, as well as the potential causation, may be warranted to understand the dynamics of these relationships better and to develop effective interventions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Statistical summaries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate statistical summaries (mean, variance, etc.)\n",
        "stat_summary = df.describe().T  # Transpose the summary for better readability\n",
        "stat_summary['variance'] = df.var() # Calculate variance for each column and add it to the statistical summary\n",
        "print(stat_summary[['mean', 'std', 'variance']]) # Display the statistical summary including mean, standard deviation, and variance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Preprocessing:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Handling Duplicates\n",
        "\n",
        "Duplicate rows in a dataset can introduce redundancy, skewing the analysis and leading to inaccurate results. In this step, we checked for and removed any duplicate rows using the duplicated() function. This ensures that each data point is unique, preserving the quality and integrity of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Handling Duplicates\n",
        "num_duplicates = df.duplicated().sum()\n",
        "print(\"Number of duplicate rows:\", num_duplicates)\n",
        "\n",
        "# Remove duplicates and save the cleaned dataset\n",
        "data = df.drop_duplicates()\n",
        "\n",
        "# Save after handling duplicates\n",
        "data.to_csv('Cleaned_Dataset.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Result:\n",
        "\n",
        "Upon checking the dataset, the result showed 0 duplicate rows, meaning that no redundant data points were found. The dataset is clean, and no further action regarding duplicates was necessary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Missing values:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for missing values\n",
        "missing_values = df.isnull().sum() # This creates a Series containing the count of missing values per column\n",
        "print(\"Missing values per column:\") # Print a message indicating that missing values will be displayed\n",
        "print(missing_values) # Print missing values per column"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Handling Outliers:\n",
        "\n",
        "Outliers are extreme values that can impact the accuracy of data analysis. To address this, we used the Interquartile Range (IQR) method, which identifies outliers by looking for values significantly above or below the normal range in numeric columns. Instead of removing these outliers, we cap their values to minimize their influence while preserving the overall dataset structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = pd.read_csv('Cleaned_Dataset.csv')\n",
        "import numpy as np\n",
        "\n",
        "# Outlier handling using IQR method\n",
        "outlier_threshold = 1.5\n",
        "\n",
        "def count_outliers(column_data):\n",
        "    q1 = np.percentile(column_data, 25)\n",
        "    q3 = np.percentile(column_data, 75)\n",
        "    iqr = q3 - q1\n",
        "    upper_bound = q3 + outlier_threshold * iqr\n",
        "    lower_bound = q1 - outlier_threshold * iqr\n",
        "    outliers = (column_data > upper_bound) | (column_data < lower_bound)\n",
        "    return sum(outliers)\n",
        "\n",
        "# Select numeric columns\n",
        "numeric_columns = data.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "# Detect outliers in each numeric column\n",
        "outlier_counts = {}\n",
        "total_rows_with_outliers = 0\n",
        "\n",
        "for column in numeric_columns:\n",
        "    outliers = count_outliers(data[column])\n",
        "    outlier_counts[column] = outliers\n",
        "    total_rows_with_outliers += outliers\n",
        "\n",
        "# Print outlier summary\n",
        "print(\"Outlier Counts:\")\n",
        "for column, count in outlier_counts.items():\n",
        "    print(f\"{column}: {count} rows with outliers\")\n",
        "\n",
        "print(f\"Total Rows with Outliers: {total_rows_with_outliers}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Result:\n",
        "\n",
        "The result shows that most variables have no outliers, indicating well-distributed data. However, noise_level (173 rows), study_load (165 rows), and living_conditions (62 rows) contain a significant number of outliers, suggesting unusual variations in these columns. These outliers may require further investigation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### handling outliers:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "data = pd.read_csv('Cleaned_Dataset.csv')\n",
        "\n",
        "\n",
        "outlier_threshold = 1.5\n",
        "\n",
        "\n",
        "def count_outliers(column_data):\n",
        "    column_data = column_data.dropna() \n",
        "    q1 = np.percentile(column_data, 25)\n",
        "    q3 = np.percentile(column_data, 75)\n",
        "    iqr = q3 - q1\n",
        "    upper_bound = q3 + outlier_threshold * iqr\n",
        "    lower_bound = q1 - outlier_threshold * iqr\n",
        "    outliers = (column_data > upper_bound) | (column_data < lower_bound)\n",
        "    return sum(outliers)\n",
        "\n",
        "\n",
        "numeric_columns = data.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "outlier_counts = {}\n",
        "total_rows = len(data)\n",
        "\n",
        "for column in numeric_columns:\n",
        "   \n",
        "    outliers = count_outliers(data[column])\n",
        "    outlier_counts[column] = outliers\n",
        "\n",
        "    \n",
        "    non_na_data = data[column].dropna()\n",
        "\n",
        "    \n",
        "    q1 = np.percentile(non_na_data, 25)\n",
        "    q3 = np.percentile(non_na_data, 75)\n",
        "    iqr = q3 - q1\n",
        "    upper_bound = q3 + outlier_threshold * iqr\n",
        "    lower_bound = q1 - outlier_threshold * iqr\n",
        "    \n",
        "   \n",
        "    data[column] = np.clip(data[column], lower_bound, upper_bound)\n",
        "\n",
        "\n",
        "data.to_csv('Cleaned_Dataset.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Checking the results by counting outliers after handling them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = pd.read_csv('Cleaned_Dataset.csv')\n",
        "import numpy as np\n",
        "\n",
        "# Outlier handling using IQR method\n",
        "outlier_threshold = 1.5\n",
        "\n",
        "def count_outliers(column_data):\n",
        "    q1 = np.percentile(column_data, 25)\n",
        "    q3 = np.percentile(column_data, 75)\n",
        "    iqr = q3 - q1\n",
        "    upper_bound = q3 + outlier_threshold * iqr\n",
        "    lower_bound = q1 - outlier_threshold * iqr\n",
        "    outliers = (column_data > upper_bound) | (column_data < lower_bound)\n",
        "    return sum(outliers)\n",
        "\n",
        "# Select numeric columns\n",
        "numeric_columns = data.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "# Detect outliers in each numeric column\n",
        "outlier_counts = {}\n",
        "total_rows_with_outliers = 0\n",
        "\n",
        "for column in numeric_columns:\n",
        "    outliers = count_outliers(data[column])\n",
        "    outlier_counts[column] = outliers\n",
        "    total_rows_with_outliers += outliers\n",
        "\n",
        "# Print outlier summary\n",
        "print(\"Outlier Counts:\")\n",
        "for column, count in outlier_counts.items():\n",
        "    print(f\"{column}: {count} rows with outliers\")\n",
        "\n",
        "print(f\"Total Rows with Outliers: {total_rows_with_outliers}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Transformation\n",
        "\n",
        "\n",
        "Encoding was not used because it is only necessary for categorical or textual data. Since the dataset contained numerical data, there was no need for encoding.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Normalization\n",
        "In the transformation process, Normalization was applied because the data consisted of numerical values that needed to be scaled to a specific range (typically 0 to 1). This ensures that features with different scales do not disproportionately influence machine learning models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "data1 = pd.read_csv('Cleaned_Dataset.csv')\n",
        "data1 = pd.DataFrame(data1)\n",
        "# Columns to normalize\n",
        "columns_to_normalize = [\n",
        "    'anxiety_level', 'self_esteem', 'depression', 'blood_pressure', \n",
        "    'sleep_quality', 'breathing_problem', 'noise_level', 'living_conditions', \n",
        "    'study_load', 'future_career_concerns', 'social_support', 'peer_pressure', \n",
        "    'extracurricular_activities', 'bullying', 'stress_level'\n",
        "]\n",
        "\n",
        "# Apply Decimal scaling normalization\n",
        "for column in columns_to_normalize:\n",
        "    max_abs_value = data1[column].abs().max()\n",
        "    data1[column] = data1[column] / (10 ** len(str(int(max_abs_value))))\n",
        "\n",
        "# Output the normalized data\n",
        "print(data1.head())\n",
        "\n",
        "# Save the normalized dataset\n",
        "data1.to_csv('Cleaned_Dataset.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Result:\n",
        "\n",
        "As seen in the table, the values for variables such as anxiety_level, self_esteem, blood_pressure, and others have been normalized. For example, blood_pressure now ranges between 0 and 1, ensuring consistent scaling across all features. This allows for more balanced analysis and model training without certain features dominating due to larger magnitudes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Aggregation\n",
        "\n",
        "Aggregation is a technique used to summarize data by grouping it based on specific categories. In this step, we grouped the dataset by stress_level and calculated the mean for numeric columns like anxiety_level, depression, and self_esteem. For categorical variables like bullying, we summed the values to provide insight into the total occurrences within each stress level group."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "data1 = pd.read_csv('Cleaned_Dataset.csv')\n",
        "# Step 5: Aggregation based on stress_level\n",
        "aggregated_df = data1.groupby('stress_level').agg({\n",
        "    'anxiety_level': 'mean',  \n",
        "    'depression': 'mean',    \n",
        "    'self_esteem': 'mean',    \n",
        "    'bullying': 'sum'  # Example of sum for categorical variables\n",
        "})\n",
        "\n",
        "\n",
        "\n",
        "# Output aggregated data\n",
        "print(\"Aggregated data:\")\n",
        "print(aggregated_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Result:\n",
        "\n",
        "The aggregated data shows the mean values of anxiety_level, depression, and self_esteem for each stress_level group, as well as the total number of bullying incidents. For instance, individuals with a stress_level of 2 have a higher mean anxiety_level (16.40) and depression (19.83) compared to those with a stress_level of 0. This helps identify patterns and correlations between stress levels and various psychological and behavioral metric"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Discretization\n",
        "\n",
        "Discretization is a process where continuous data is divided into discrete categories or bins. In this step, we transformed the anxiety_level variable into three categories: Low, Medium, and High. This simplifies the data and makes it easier to analyze trends across different levels of anxiety."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "data1 = pd.read_csv('Cleaned_Dataset.csv')\n",
        "\n",
        "\n",
        "# Discretization of anxiety_level into categories\n",
        "data1['anxiety_level'] = pd.cut(data1['anxiety_level'], bins=3, labels=['Low', 'Medium', 'High'])\n",
        "\n",
        "# Save the discretized dataset\n",
        "data1.to_csv('Cleaned_Dataset.csv', index=False)\n",
        "\n",
        "# Display the first few rows\n",
        "print(\"Data after discretization:\")\n",
        "print(data1[['anxiety_level', 'anxiety_level']].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Result:\n",
        "\n",
        "The anxiety_level column has been discretized into the bins Low, Medium, and High. For example, a anxiety_level of 14 is classified as Medium, while a value of 16 is classified as High. This categorization helps in interpreting the data more intuitively and allows for easier comparisons across different groups."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Feature Selection\n",
        "The number of available features is 20, and the feature selection includes blood pressure, sleep quality, future career concerns, bullying, and stress level.\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('Cleaned_Dataset.csv')\n",
        "\n",
        "# Separate features from the target variable\n",
        "X = df.drop(columns=['anxiety_level', 'anxiety_level_binned'])  # Exclude non-numeric columns\n",
        "y = df['anxiety_level']\n",
        "\n",
        "# Select only numeric columns for feature selection\n",
        "X_numeric = X.select_dtypes(include=[float, int])\n",
        "\n",
        "# Check the number of features\n",
        "n_features = X_numeric.shape[1]\n",
        "print('Number of features available:', n_features)\n",
        "\n",
        "# Specify the number of features to choose\n",
        "num_features_to_select = min(5, n_features)  # Choose the least between 5 and the actual number of features\n",
        "selector = SelectKBest(score_func=f_classif, k=num_features_to_select)\n",
        "\n",
        "# Apply feature selection\n",
        "X_selected = selector.fit_transform(X_numeric, y)\n",
        "\n",
        "# Get selected feature indicators\n",
        "selected_indices = selector.get_support(indices=True)\n",
        "\n",
        "# Get selected feature names\n",
        "selected_features = X_numeric.columns[selected_indices]\n",
        "\n",
        "print('Selected Features:', selected_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Load data\n",
        "df=pd.read_csv('Cleaned_Dataset.csv')\n",
        "print(df)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
